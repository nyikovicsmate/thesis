{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "input_pipeline_benchmark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iPk0FsT3xzKy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdYZfI72x6LD",
        "colab_type": "text"
      },
      "source": [
        "## 1. Preparation\n",
        "\n",
        "Download and preprocess the BDD100k dataset, the steps equal the ones found in the [bdd100k.ipynb](https://github.com/nyikovicsmate/thesis/blob/dev/utils/datasets/bdd100k.ipynb) dataset preparation notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ8IHjdx75qa",
        "colab_type": "code",
        "outputId": "b74353e5-23b4-4263-9edc-a1ae979b04be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# get the dynamic download link\n",
        "!curl -s \"https://2x5kv9t5uf.execute-api.us-west-2.amazonaws.com/production?func=create_download_challenge_link&filename=bdd100k\"%\"2Fbdd100k_images.zip\" -H \"Accept: */*\" -o uri.txt\n",
        "# download the dataset (approx 6.5G)\n",
        "!xargs -n 1 curl -o \"bdd100k_images.zip\" < uri.txt\n",
        "# extract\n",
        "!unzip -q bdd100k_images.zip -d bdd100k_images\n",
        "\n",
        "# if there is a problem with unzipping it's most likely caused by a failed download\n",
        "# this can happen when colab is a little too slow to start the download and the dynamic download link expires\n",
        "# if this happens just try running the cell again"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 6619M  100 6619M    0     0  36.1M      0  0:03:03  0:03:03 --:--:-- 36.5M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQZHUKe18YrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move things around\n",
        "!mv ./bdd100k_images/bdd100k/images/100k ./images "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCMSmpjhNb9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleanup\n",
        "!rm uri.txt\n",
        "!rm bdd100k_images.zip\n",
        "!rm -rf ./bdd100k_images\n",
        "!rm -rf sample_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z80N9oPONb4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the preprocesing script\n",
        "!curl -s -O https://raw.githubusercontent.com/nyikovicsmate/thesis/dev/utils/preprocess.py\n",
        "# download requirements.txt\n",
        "!curl -s -O https://raw.githubusercontent.com/nyikovicsmate/thesis/dev/utils/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1XEvXubNbv2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "4f63ac67-72d1-4fd2-a424-18bc3bda8862"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python>=4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (4.1.2.30)\n",
            "Collecting numpy>=1.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 232kB/s \n",
            "\u001b[?25hCollecting tqdm>=4.43\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/55/fd9170ba08a1a64a18a7f8a18f088037316f2a41be04d2fe6ece5a653e8f/tqdm-4.43.0-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n",
            "\u001b[?25hCollecting h5py>=2.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb>=0.98 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.98)\n",
            "Collecting rawpy>=0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/50/13dd9863a3e30b10f15e5abe7c1545db24a78cfe820c342978ae5d87e8c3/rawpy-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.10->-r requirements.txt (line 4)) (1.12.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tqdm, h5py, rawpy\n",
            "  Found existing installation: numpy 1.17.5\n",
            "    Uninstalling numpy-1.17.5:\n",
            "      Successfully uninstalled numpy-1.17.5\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "  Found existing installation: h5py 2.8.0\n",
            "    Uninstalling h5py-2.8.0:\n",
            "      Successfully uninstalled h5py-2.8.0\n",
            "Successfully installed h5py-2.10.0 numpy-1.18.1 rawpy-0.14.0 tqdm-4.43.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khEdVEzmNf1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "36a5276f-9976-41e6-a609-abec51887269"
      },
      "source": [
        "!python3 preprocess.py -h"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: preprocess.py [-h] [-a AUGMENT_VALUE] [-f {png,hdf,lmdb}] [-g]\n",
            "                     [-m {clip,clip_rnd,scale,scale_rnd}] [-n NAME]\n",
            "                     [-s SIZE SIZE]\n",
            "                     [root]\n",
            "\n",
            "positional arguments:\n",
            "  root                  The root directory from where the search for images\n",
            "                        starts. (default: '.)'\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -a AUGMENT_VALUE, --augment AUGMENT_VALUE\n",
            "                        Besides preprocessed images, store augmented ones as\n",
            "                        well. Augmented image is a processed image with every\n",
            "                        2nd pixel (in a checkerboard pattern) set to\n",
            "                        augment_value [0-255].\n",
            "  -f {png,hdf,lmdb}, --format {png,hdf,lmdb}\n",
            "                        Output format to use. Supported: png, hdf, lmdb.\n",
            "                        (default: png)\n",
            "  -g, --grayscale       Grayscale images.\n",
            "  -m {clip,clip_rnd,scale,scale_rnd}, --method {clip,clip_rnd,scale,scale_rnd}\n",
            "                        Processing method to use. (default: scale)\n",
            "  -n NAME, --name NAME  Name of the output dataset file/directory. (default:\n",
            "                        'dataset')\n",
            "  -s SIZE SIZE, --size SIZE SIZE\n",
            "                        Output size of images [height, width]. (default: [225,\n",
            "                        225])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpt_S8h0Nhfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "68f046cb-9b34-4cc3-e64d-1f424c7a7526"
      },
      "source": [
        "# preprocess the dataset\n",
        "!python3 preprocess.py -n \"bdd100k_hdf\" -f \"hdf\" -m \"clip\" -g -s 225 225 \n",
        "!python3 preprocess.py -n \"bdd100k_lmdb\" -f \"lmdb\" -m \"clip\" -g -s 225 225 \n",
        "!python3 preprocess.py -n \"bdd100k_png\" -f \"png\" -m \"clip\" -g -s 225 225 "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking for images under /content\n",
            "Found 100000 images.\n",
            "Processing images.\n",
            "100% 100000/100000 [23:02<00:00, 72.34it/s]\n",
            "Done.\n",
            "Looking for images under /content\n",
            "Found 100000 images.\n",
            "Processing images.\n",
            "100% 100000/100000 [32:14<00:00, 51.69it/s]\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VujUgifeOJtX",
        "colab_type": "text"
      },
      "source": [
        "## 2. Benchmarking\n",
        "\n",
        "This point all 3 formats contain the same 100000 225x225 px grayscale images. The benchmarks are focused on the following 4 details of each format:\n",
        "\n",
        "1.   the **disk size** of the dataset\n",
        "2.   random **single image** read speed\n",
        "3.   **sequential batch** read speed \n",
        "4.   **random batch** read speed\n",
        "\n",
        "\n",
        "1.   **disk size benchmark**: calculating the hard disk space the dataset in the given format occupies.\n",
        "2.   **single image read benchmark**: given 1000 random indexes in the range of (0, 100000) measuring how long does it take on average (from 100 runs) to read one.\n",
        "3. **sequential batch benchmark**: with a batch size of 1000 indexes, and with the indexes being sequentially ordered (e.g. 1st batch (0-999), 2nd (1000-1999) and so on), measuring how long does it take to read the whole dataset into memory once.\n",
        "4. **random batch benchmark**: with a batch size of 1000 indexes, and the indexes in each batch are in a random order, measuring how long does it take to read the whole dataset into memory once.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw-WhYrR8kYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib\n",
        "import h5py\n",
        "import lmdb\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pickle\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "### helper functions\n",
        "\n",
        "\n",
        "def get_size(path: pathlib.Path):\n",
        "    \"\"\"\n",
        "    Returns the size of a file/directory in MB.\n",
        "    \"\"\"\n",
        "    if path.is_file():\n",
        "        return path.stat().st_size / (1024**2) # st_size returns size in bytes\n",
        "    elif path.is_dir():\n",
        "        return sum(f.stat().st_size / (1024**2) for f in path.rglob('*') if f.is_file())\n",
        "    else:\n",
        "        raise Exception()\n",
        "\n",
        "def get_random_batch() -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns a 1000 indexes from the range of (0,100000).\n",
        "    \"\"\"\n",
        "    return np.random.randint(0, 100000, 1000)\n",
        "\n",
        "def get_sequential_batches() -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns a 100 batches of 1000 sequential indexes covering the range of (0,100000).\n",
        "    \"\"\"\n",
        "    idxs = np.arange(0, 100000)\n",
        "    batches = np.reshape(idxs, (100, 1000))\n",
        "    return batches\n",
        "\n",
        "def get_random_batches() -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns a 100 batches of 1000 random indexes covering the range of (0,100000).\n",
        "    \"\"\"\n",
        "    idxs = np.arange(0, 100000)\n",
        "    np.random.shuffle(idxs)\n",
        "    batches = np.reshape(idxs, (100, 1000))\n",
        "    return batches\n",
        "\n",
        "\n",
        "### single image read functions \n",
        "\n",
        "\n",
        "def read_single_png(idx: int):\n",
        "    \"\"\"\n",
        "    Utility function for reading an image back into memory.\n",
        "    \"\"\"\n",
        "    image_path = pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_png\", \"images\", f\"{idx}.png\")\n",
        "    return np.array(cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE))\n",
        "\n",
        "def read_single_hdf(idx: int):\n",
        "    \"\"\"\n",
        "    Utility function for reading an image back into memory from a .h5 file.\n",
        "    \"\"\"\n",
        "    with h5py.File(pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_hdf.h5\"), \"r\") as file:\n",
        "        image = np.array(file[\"images\"][idx], dtype=np.uint8)\n",
        "    return image\n",
        "\n",
        "def read_single_lmdb(idx: int):\n",
        "    \"\"\"\n",
        "    Utility function for reading an image back into memory from a lmdb database.\n",
        "    \"\"\"\n",
        "    lmdb_dir = pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_lmdb\")\n",
        "    env = lmdb.open(str(lmdb_dir), readonly=True, max_dbs=2, readahead=False)\n",
        "    db = env.open_db(key=\"images\".encode(\"utf8\"))\n",
        "    with env.begin(db=db) as txn:\n",
        "        data = txn.get(f\"{idx}\".encode(\"utf8\"))\n",
        "        image = pickle.loads(data)\n",
        "    env.close()\n",
        "    return image\n",
        "\n",
        "\n",
        "### sequential batch read functions\n",
        "\n",
        "\n",
        "def read_sequential_png(idxs: List[int]):\n",
        "    return [read_single_png(idx) for idx in idxs]\n",
        "\n",
        "def read_sequential_hdf(idxs: List[int]):\n",
        "    with h5py.File(pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_hdf.h5\"), \"r\") as file:\n",
        "        # h5py supports index ranges, read times are significantly faster \n",
        "        # than reading each image separately \n",
        "        images = np.array(file[\"images\"][idxs], dtype=np.uint8)\n",
        "    return images\n",
        "\n",
        "def read_sequential_lmdb(idxs: List[int]):\n",
        "    lmdb_dir = pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_lmdb\")\n",
        "    images = []\n",
        "    # set readahead to True to fully utilize underlying os capabilities  \n",
        "    env = lmdb.open(str(lmdb_dir), readonly=True, max_dbs=2, readahead=True) \n",
        "    db = env.open_db(key=\"images\".encode(\"utf8\"))\n",
        "    with env.begin(db=db) as txn:\n",
        "        for idx in idxs:\n",
        "            data = txn.get(f\"{idx}\".encode(\"utf8\"))\n",
        "            image = pickle.loads(data)\n",
        "            images.append(image)\n",
        "    env.close()\n",
        "    return images\n",
        "\n",
        "### random batch read functions \n",
        "\n",
        "def read_random_png(idxs: List[int]):\n",
        "    return read_sequential_png(idxs)\n",
        "\n",
        "def read_random_hdf(idxs: List[int]):\n",
        "    idxs = list(sorted(idxs))\n",
        "    with h5py.File(pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_hdf.h5\"), \"r\") as file:\n",
        "        # h5py still supports index ranges, read times are significantly faster \n",
        "        # than reading each image separately \n",
        "        # but with random indexes the indexes must be in ascending order\n",
        "        images = np.array(file[\"images\"][idxs], dtype=np.uint8)\n",
        "    return images\n",
        "\n",
        "def read_random_lmdb(idxs: List[int]):\n",
        "    lmdb_dir = pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_lmdb\")\n",
        "    images = []\n",
        "    # set readahead to False to speed up random reads  \n",
        "    env = lmdb.open(str(lmdb_dir), readonly=True, max_dbs=2, readahead=False) \n",
        "    db = env.open_db(key=\"images\".encode(\"utf8\"))\n",
        "    with env.begin(db=db) as txn:\n",
        "        for idx in idxs:\n",
        "            data = txn.get(f\"{idx}\".encode(\"utf8\"))\n",
        "            image = pickle.loads(data)\n",
        "            images.append(image)\n",
        "    env.close()\n",
        "    return images\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDqgY979eYUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def size_benchmark():\n",
        "    results = {\"png\": 0, \"hdf\": 0, \"lmdb\": 0}\n",
        "    results[\"png\"] = get_size(pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_png\", \"images\"))\n",
        "    results[\"hdf\"] = get_size(pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_hdf.h5\"))\n",
        "    results[\"lmdb\"] = get_size(pathlib.Path.joinpath(pathlib.Path.cwd(), \"bdd100k_lmdb\"))\n",
        "    return results\n",
        "\n",
        "def single_benchmark():\n",
        "    results = {\"png\": 0, \"hdf\": 0, \"lmdb\": 0}\n",
        "    idxs = get_random_batch()[0:100] # 100 random indexes\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_single_png(idx)\n",
        "        end = time.time()\n",
        "        results[\"png\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"png\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_single_hdf(idx)\n",
        "        end = time.time()\n",
        "        results[\"hdf\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"hdf\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_single_lmdb(idx)\n",
        "        end = time.time()\n",
        "        results[\"lmdb\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"lmdb\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    return results\n",
        "\n",
        "def sequential_batch_benchmark():\n",
        "    results = {\"png\": 0, \"hdf\": 0, \"lmdb\": 0}\n",
        "    idxs = get_sequential_batches()\n",
        "\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_sequential_png(idx)\n",
        "        end = time.time()\n",
        "        results[\"png\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"png\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_sequential_hdf(idx)\n",
        "        end = time.time()\n",
        "        results[\"hdf\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"hdf\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_sequential_lmdb(idx)\n",
        "        end = time.time()\n",
        "        results[\"lmdb\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"lmdb\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def random__batch_benchmark():\n",
        "    results = {\"png\": 0, \"hdf\": 0, \"lmdb\": 0}\n",
        "    idxs = get_sequential_batches()\n",
        "\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_random_png(idx)\n",
        "        end = time.time()\n",
        "        results[\"png\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"png\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_random_hdf(idx)\n",
        "        end = time.time()\n",
        "        results[\"hdf\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"hdf\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    for idx in idxs:\n",
        "        start = time.time()\n",
        "        read_random_lmdb(idx)\n",
        "        end = time.time()\n",
        "        results[\"lmdb\"] += (end-start) * 1000 # time.time is in sec\n",
        "    results[\"lmdb\"] /= 100 # take the average of 100 runs\n",
        "\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMz_yJlSjiYw",
        "colab_type": "text"
      },
      "source": [
        "## 3. Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twGCvswUjpyA",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Size benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yengAw3Ri2eh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bc91775c-e9e9-4a74-8963-bc361d7d6269"
      },
      "source": [
        "print(\"Results of size benchmark [MB]\")\n",
        "print(size_benchmark())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of size benchmark [MB]\n",
            "{'png': 1693.4731349945068, 'hdf': 4827.977561950684, 'lmdb': 5082.4375}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTFPqtnJjInx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "88120446-1087-4382-f67f-687f77de737e"
      },
      "source": [
        "!du -h bdd100k_png/"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9G\tbdd100k_png/images\n",
            "1.9G\tbdd100k_png/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKCoWL7-jQF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0951022-f490-4d0f-e002-43a664b5648f"
      },
      "source": [
        "!du -h bdd100k_hdf.h5"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.8G\tbdd100k_hdf.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQF6hebJjUH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2778d304-ef03-458e-baec-0a558e4bd87f"
      },
      "source": [
        "!du -h bdd100k_lmdb"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0G\tbdd100k_lmdb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DOnDF9sjuj4",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Single read benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yUesMYhjgat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ffe74f9a-6a63-4f4e-b6c9-5116248f2d48"
      },
      "source": [
        "print(\"Results of single read benchmark [ms]\")\n",
        "print(single_benchmark())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of single read benchmark [ms]\n",
            "{'png': 0.7945394515991211, 'hdf': 4.191272258758545, 'lmdb': 4.199466705322266}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6PbR4b_jz8k",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Sequential batch read benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMeb-Tr6jzeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8c88ef16-d3ce-4608-f2a0-cd090dc0bbe2"
      },
      "source": [
        "print(\"Results of sequential batch read benchmark [ms]\")\n",
        "print(sequential_batch_benchmark())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of sequential batch read benchmark [ms]\n",
            "{'png': 690.3145956993103, 'hdf': 705.4850220680237, 'lmdb': 966.8847441673279}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUUQl25Sj6YW",
        "colab_type": "text"
      },
      "source": [
        "### 3.4 Random batch read benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9P13SSxjzbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5c35a4ba-b5f3-4842-fcbe-8008e1708fc3"
      },
      "source": [
        "print(\"Results of random batch read benchmark [ms]\")\n",
        "print(random__batch_benchmark())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of random batch read benchmark [ms]\n",
            "{'png': 1365.8288359642029, 'hdf': 742.2910022735596, 'lmdb': 3947.6974749565125}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPk0FsT3xzKy",
        "colab_type": "text"
      },
      "source": [
        "## Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKUUqW60xsUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "result_single = {\"png\": 0, \"hdf5\": 0, \"lmdb\": 0}\n",
        "result_seq = {\"png\": 0, \"hdf5\": 0, \"lmdb\": 0}\n",
        "result_rand = {\"png\": 0, \"hdf5\": 0, \"lmdb\": 0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IKgHZrSyAkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "before = time.time()\n",
        "read_single_png(0)\n",
        "after = time.time()\n",
        "result_single[\"png\"] = (after - before) * 1000\n",
        "\n",
        "before = time.time()\n",
        "read_single_hdf5(0)\n",
        "after = time.time()\n",
        "result_single[\"hdf5\"] = (after - before) * 1000\n",
        "\n",
        "before = time.time()\n",
        "read_single_lmdb(0)\n",
        "after = time.time()\n",
        "result_single[\"lmdb\"] = (after - before) * 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "052E6SRVy-uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "before = time.time()\n",
        "read_many_sequentially_png()\n",
        "after = time.time()\n",
        "result_seq[\"png\"] = (after - before) * 1000\n",
        "\n",
        "before = time.time()\n",
        "read_many_sequentially_hdf5()\n",
        "after = time.time()\n",
        "result_seq[\"hdf5\"] = (after - before) * 1000\n",
        "\n",
        "before = time.time()\n",
        "read_many_sequentially_lmdb()\n",
        "after = time.time()\n",
        "result_seq[\"lmdb\"] = (after - before) * 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdX7t82JzMqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "before = time.time()\n",
        "read_many_randomly_png(random_idxs)\n",
        "after = time.time()\n",
        "result_rand[\"png\"] = (after - before) * 1000\n",
        "\n",
        "idxs = sorted(list(random_idxs))\n",
        "before = time.time()\n",
        "read_many_randomly_hdf5(idxs)\n",
        "after = time.time()\n",
        "result_rand[\"hdf5\"] = (after - before) * 1000\n",
        "\n",
        "before = time.time()\n",
        "read_many_randomly_lmdb(random_idxs)\n",
        "after = time.time()\n",
        "result_rand[\"lmdb\"] = (after - before) * 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b63XjhULyF3T",
        "colab_type": "code",
        "outputId": "16ab4ef0-7364-41b1-8b48-d19d644f6bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(f\"Single read results [ms]: {result_single}\")\n",
        "print(f\"Sequential read results [ms]: {result_seq}\")\n",
        "print(f\"Random read results [ms]: {result_rand}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single read results [ms]: {'png': 0.41174888610839844, 'hdf5': 1.4827251434326172, 'lmdb': 0.3237724304199219}\n",
            "Sequential read results [ms]: {'png': 11.526823043823242, 'hdf5': 1.3456344604492188, 'lmdb': 0.9093284606933594}\n",
            "Random read results [ms]: {'png': 12.545347213745117, 'hdf5': 3.5293102264404297, 'lmdb': 1.1408329010009766}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}